[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quarto Website",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nEsto es hacer una prueba"
  },
  {
    "objectID": "Python.html",
    "href": "Python.html",
    "title": "Python",
    "section": "",
    "text": "Information about Python"
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Python",
    "section": "",
    "text": "Python is a popular and versatile programming language known for its simplicity and readability. Here’s a simple introduction highlighting the key elements to consider when using this programming language:\n\nReadability: Python focuses on code readability, meaning its syntax is clear and easy to understand. This makes it ideal for both beginners and experienced programmers.\nInterpreted: Python is an interpreted language, which means you don’t need to compile your code before running it. This makes writing and testing code easier, as you can see results immediately.\nDocumentation and Comments: Python supports inline comments and docstrings, allowing developers to document their code effectively. Writing clear and concise comments helps explain complex logic, assumptions, and intentions, enhancing readability and maintainability.\nDynamic typing: Python is dynamically typed, so you don’t need to explicitly declare a variable’s type. Python infers the data type during program execution.\n\n# Integer\nx = 10  \n# Output: &lt;class 'int'&gt;\nprint(type(x))  \n\n# String\nx = \"Hello\" \n# Output: &lt;class 'str'&gt; \nprint(type(x))  \n\n# List\nx = [1, 2, 3] \n# Output: &lt;class 'list'&gt; \nprint(type(x))                                           \n\nCross-platform: Python is compatible with various operating systems such as Windows, macOS, and Linux, making it highly portable.\nExtensive standard library: Python comes with an extensive standard library that provides modules and functions for a variety of tasks, from file manipulation to web development and data processing.\nObject-oriented: Python is an object-oriented programming language, meaning everything in Python is an object with properties and methods.\nIndentation: Indentation is the whitespace at the beginning of a line of code. Python uses indentation to delimit code blocks instead of braces or keywords like in other languages. This promotes writing clean and readable code.\n\nif x &gt; 5:\n    print(\"This is a Python code example\")\n    print(\"Great!\")\nelse:\n    print(\"Oh no!\")                                           \n\nActive community: Python has a large community of developers who contribute libraries, tutorials, and helpful resources. This means you can easily find help and access a wealth of educational resources."
  },
  {
    "objectID": "neuralNetworks.html",
    "href": "neuralNetworks.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Introduction to Neural Networks"
  },
  {
    "objectID": "Proyecto_HeadCounting.html",
    "href": "Proyecto_HeadCounting.html",
    "title": "Librerias",
    "section": "",
    "text": "!pip install --upgrade tensorflow\n!pip install timm\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport os\nimport concurrent.futures\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport glob\nimport scipy.io\n\nfrom google.colab import drive\n\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\n\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "Proyecto_HeadCounting.html#imagenes-de-grupos",
    "href": "Proyecto_HeadCounting.html#imagenes-de-grupos",
    "title": "Librerias",
    "section": "Imagenes de grupos",
    "text": "Imagenes de grupos\n\ndef cantidad_personas(lista):\n    personas = 0\n    for elem in lista:\n        if elem.get('label')=='person':\n            personas += 1\n    return personas\n\ndef imprimir_objetos(img, lista):\n    plt.imshow(img.values[0])\n    for elem in lista:\n        if elem.get('label')=='person':\n            width = elem['box']['xmax'] - elem['box']['xmin']\n            height = elem['box']['ymax'] - elem['box']['ymin']\n            rect = patches.Rectangle((elem['box']['xmin'], elem['box']['ymin']), width, height, linewidth=5, edgecolor='r', facecolor='none')\n\n            plt.gca().add_patch(rect)\n\n    plt.title(f'Cantidad de personas: {cantidad_personas(lista)}', fontsize=60)\n    plt.axis('off')\n\ndef array_to_image(array):\n    return Image.fromarray((array * 255).astype('uint8'))\n\n\nrandom = df.sample(1)\nimagen_array = random['imagen'].values[0]\nimagen_pil = array_to_image(imagen_array)\n\nev_modelo = model(imagen_pil)\n\ncantidad = cantidad_personas(ev_modelo)\n\nprint(f\"Cantidad de personas: {cantidad}\")\n\nprint(ev_modelo)\n\nplt.figure(figsize=(25, 25))\nimprimir_objetos(random['imagen'], ev_modelo)\n\n\ndef imprimir_objetos_multiple(img, lista):\n    plt.imshow(img)\n    for elem in lista:\n        if elem.get('label')=='person':\n            width = elem['box']['xmax'] - elem['box']['xmin']\n            height = elem['box']['ymax'] - elem['box']['ymin']\n            rect = patches.Rectangle((elem['box']['xmin'], elem['box']['ymin']), width, height, linewidth=0.5, edgecolor='r', facecolor='none')\n\n            plt.gca().add_patch(rect)\n\n    plt.title(f'Cantidad de personas: {cantidad_personas(lista)}')\n    plt.axis('off')\n\n\n# Ahora lo pruebo con diversas imagenes del df\nrandom = df.sample(9)\n\nplt.figure(figsize=(10, 10))\nfor i, (_, row) in enumerate(random.iterrows()):\n    ax = plt.subplot(3, 3, i + 1)\n    imagen_array = row['imagen']\n    imagen_pil = array_to_image(imagen_array)\n    ev_modelo = model(imagen_pil)\n    imprimir_objetos_multiple(row['imagen'], ev_modelo)\n\nAl ser un modelo que detecta objetos en algunos momentos puede generar algunos errores pero en general parece funcionar de forma bastante correcta"
  },
  {
    "objectID": "Proyecto_HeadCounting.html#imagenes-de-multitudes",
    "href": "Proyecto_HeadCounting.html#imagenes-de-multitudes",
    "title": "Librerias",
    "section": "Imagenes de multitudes",
    "text": "Imagenes de multitudes\nAhora lo voy a poner a prueba con imagenes de multitudes obtenidas originalmente de kaggle del dataset ShangaiTech https://www.kaggle.com/datasets/tthien/shanghaitech\n\n#Importo algunas imagenes de multitudes\ndf_crowd = tf.keras.utils.image_dataset_from_directory('/content/gdrive/MyDrive/Colab Notebooks/ProyectoFinal/Crowd Counting/shanghaitech_with_people_density_map/ShanghaiTech/part_A/train_data/images/', shuffle=False)\n\nFound 300 files belonging to 1 classes.\n\n\n\n#La información de la cantidad de personas que se encuentra en cada foto esta almacenada en archivos .mat\n\n# Patrón para seleccionar todos los archivos .mat en el directorio\nmat_files = glob.glob(os.path.join('/content/gdrive/MyDrive/Colab Notebooks/ProyectoFinal/Crowd Counting/shanghaitech_with_people_density_map/ShanghaiTech/part_A/train_data/ground-truth', '*.mat'))\n\n# Ordenar la lista de archivos\nmat_files_sorted = sorted(mat_files)\n\n# Lista para almacenar los datos cargados\ndata_list = []\n\n# Iterar sobre los archivos .mat de train y cargarlos\nfor mat_file in mat_files_sorted:\n    mat_data = scipy.io.loadmat(mat_file)\n    data_list.append(mat_data)\n\n\nimage_info_df = []\nfor data in data_list:\n    image_info = data[\"image_info\"]\n    image_info_df.append(image_info)\n\n\nnumber_list = np. array([])\nfor image in image_info_df:\n    number_array = image[0,0][0][0][1]\n    number = number_array[0][0]\n    number_list = np.append(number_list, number)\nnumber_list = number_list.astype(int)\n\n\n# Se muestran algunas imagenes con la cantidad de personas que la componen\nplt.figure(figsize=(7, 7))\nfor images, labels in df_crowd.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype('uint8'))\n        plt.title(number_list[i])\n        plt.axis('off')\n\nPara poder utilizar las funciones ya definidas tengo que transformar al mismo formato de df las imagenes de multitudes\n\n# Con esto df_crowd_pd es un DataFrame de pandas con una columna 'imagen' que contiene arrays de NumPy\n# y una columna 'HeadCount' que contiene las etiquetas correspondientes\n\nimagenes = []\n\nfor imagenes_batch, etiquetas_batch in df_crowd:\n    imagenes.extend(imagenes_batch.numpy())\n\ndf_crowd_pd = pd.DataFrame({'imagen': imagenes})\n\ndf_crowd_pd['HeadCount'] = pd.DataFrame(number_list) #La información de la cantidad de personas ya la tengo de number_list\n\n\ndf_crowd_pd.head()\n\n\n  \n    \n\n\n\n\n\n\nimagen\nHeadCount\n\n\n\n\n0\n[[[251.0, 250.0, 254.0], [207.0, 206.0, 211.5]...\n1546\n\n\n1\n[[[44.416992, 58.918945, 68.418945], [78.64258...\n920\n\n\n2\n[[[251.0, 253.0, 252.0], [241.5, 245.5, 246.5]...\n371\n\n\n3\n[[[39.0, 52.0, 34.0], [35.5, 48.5, 30.5], [43....\n298\n\n\n4\n[[[130.5, 104.5, 115.5], [136.5, 110.5, 121.5]...\n184\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Se enseñan 9 imagenes al azar para verificar el correcto funcionamiento del dataframe\nrandom = df_crowd_pd.sample(9)\n\nrandom['imagen'] = random['imagen'] / 255.0\n\nplt.figure(figsize=(10, 10))\nfor i, (_, row) in enumerate(random.iterrows()):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(row['imagen'])\n    plt.title(row['HeadCount'])\n    plt.suptitle('Imagenes aleatorias')\n    plt.axis('off')\n\n\ndef imprimir_multitud_multiple(data, lista):\n    img = data['imagen']\n    plt.imshow(img)\n    for elem in lista:\n        if elem.get('label')=='person':\n            width = elem['box']['xmax'] - elem['box']['xmin']\n            height = elem['box']['ymax'] - elem['box']['ymin']\n            rect = patches.Rectangle((elem['box']['xmin'], elem['box']['ymin']), width, height, linewidth=0.5, edgecolor='r', facecolor='none')\n\n            plt.gca().add_patch(rect)\n\n    title = f'Cantidad de personas calculada: {cantidad_personas(lista)}\\n'\n    title += f'Cantidad de personas real: {data[\"HeadCount\"]}'\n    plt.title(title, fontsize=10)\n    plt.axis('off')\n\n\nrandom = df_crowd_pd.sample(9)\n\nrandom['imagen'] = random['imagen'] / 255.0\n\nplt.figure(figsize=(10, 10))\nfor i, (_, row) in enumerate(random.iterrows()):\n    ax = plt.subplot(3, 3, i + 1)\n    imagen_array = row['imagen']\n    imagen_pil = array_to_image(imagen_array)\n    ev_modelo = model(imagen_pil)\n    imprimir_multitud_multiple(row, ev_modelo)\n\nComo se puede ver el resultado obtenido no es el esperado. Para intentar mejorarlo se va a proceder a dividir una imagen en varios trozos y a evaluar uno por uno\n\ndef dividir_imagen(img, size=(50, 50)):\n    # Obtiene el tamaño de la imagen\n    width, height = img.size\n\n    # Tamaño de los trozos\n    trozo_width, trozo_height = size\n\n    # Calcula la cantidad de trozos a lo largo de x e y\n    trozos_x = width // trozo_width\n    trozos_y = height // trozo_height\n\n    # Lista para almacenar los trozos\n    trozos = []\n\n    # Itera sobre los trozos y agrégalos a la lista\n    for j in range(trozos_y):\n        for i in range(trozos_x):\n            xmin, ymin = i * trozo_width, j * trozo_height\n            xmax, ymax = xmin + trozo_width, ymin + trozo_height\n            trozo = img.crop((xmin, ymin, xmax, ymax))\n            trozos.append(trozo)\n\n    return trozos\n\n\n# Para hacer esta prueba selecciono una imagen en especifico en onde se identifiquen claramente las caras de las personas\n\n# Imagen que se desea dividir\nimagen_crowd = Image.fromarray(df_crowd_pd['imagen'][5].astype('uint8'))\n\n# Tamaño de los trozos\ntamaño_trozos = (50, 50)\n\n# Obtiene los trozos de la imagen\ntrozos = dividir_imagen(imagen_crowd, tamaño_trozos)\n\n# Muestra cada trozo individualmente\nfor i, trozo in enumerate(trozos):\n    trozo.show()\n\n\nfilas = int(np.ceil(np.sqrt(len(trozos))))\ncolumnas = int(np.ceil(len(trozos) / filas))\n\n# Muestra todos los trozos en una sola imagen\nplt.figure(figsize=(10, 10))\n\nfor i, trozo in enumerate(trozos, 1):\n    plt.subplot(filas, columnas, i)\n    plt.imshow(np.array(trozo))\n    plt.axis('off')\n\nplt.show()\n\n\nimagen_crowd\n\n\n\n\n\n# Ahora aplico el modelo a cada trozo y se muestra el conteo de las personas localizadas\n\nfilas = int(np.ceil(np.sqrt(len(trozos))))\ncolumnas = int(np.ceil(len(trozos) / filas))\n\nplt.figure(figsize=(15, 15))\nfor i, trozo in enumerate(trozos, 1):\n    ax = plt.subplot(filas, columnas, i)\n    ev_modelo = model(trozo)\n    imprimir_objetos_multiple(trozo, ev_modelo)\n\nComo se puede apreciar no ha sido detectada ninguna persona. El problema puede venir de la calidad de la imagen. Para reducir el consumo de memoria las imagenes se importaron de forma automatica, sin especificar el tamaño, por lo que fueron reducidas a 256x256\nSe va a volver a realizar la prueba con otra foto de mayor calidad, tanto descargandola en 256x256 como en su resolución real\n\n# Imagen que se desea dividir\nimagen_crowd = Image.fromarray(df_crowd_pd['imagen'][7].astype('uint8'))\npersonas_real = number_list[7]\n\n# Tamaño de los trozos\ntamaño_trozos = (50, 50)\n\n# Obtiene los trozos de la imagen\ntrozos = dividir_imagen(imagen_crowd, tamaño_trozos)\n\n# Muestra cada trozo individualmente\nfor i, trozo in enumerate(trozos):\n    trozo.show()\n\n\nfilas = int(np.ceil(np.sqrt(len(trozos))))\ncolumnas = int(np.ceil(len(trozos) / filas))\n\n# Muestra todos los trozos en una sola imagen\nplt.figure(figsize=(10, 10))\n\nfor i, trozo in enumerate(trozos, 1):\n    plt.subplot(filas, columnas, i)\n    plt.imshow(np.array(trozo))\n    plt.axis('off')\n\nplt.show()\n\n\nimagen_crowd\n\n\n\n\n\nfilas = int(np.ceil(np.sqrt(len(trozos))))\ncolumnas = int(np.ceil(len(trozos) / filas))\n\npersonas_foto = 0\n\nplt.figure(figsize=(15, 15))\nfor i, trozo in enumerate(trozos, 1):\n    ax = plt.subplot(filas, columnas, i)\n    ev_modelo = model(trozo)\n    imprimir_objetos_multiple(trozo, ev_modelo)\n    personas_foto = personas_foto + cantidad_personas(ev_modelo)\n\nprint(f'El total calculado de personas en la imagen es: {personas_foto}')\nprint(f'El total real de personas en la imagen es: {personas_real}')\n\n\nimagen_tf = tf.keras.preprocessing.image.load_img('/content/gdrive/MyDrive/Colab Notebooks/ProyectoFinal/Crowd Counting/shanghaitech_with_people_density_map/ShanghaiTech/part_A/train_data/images/Crowd/IMG_105.jpg')\n\n\nimagen_np = tf.keras.preprocessing.image.img_to_array(imagen_tf)\n\n\n# Imagen que se desea dividir\nimagen_crowd = Image.fromarray(imagen_np.astype('uint8'))\nprint(imagen_crowd.size)\npersonas_real = number_list[7]\n\n# Tamaño de los trozos\ntamaño_trozos = (200, 200)\n\n# Obtiene los trozos de la imagen\ntrozos = dividir_imagen(imagen_crowd, tamaño_trozos)\n\n# Muestra cada trozo individualmente\nfor i, trozo in enumerate(trozos):\n    trozo.show()\n\n(990, 632)\n\n\n\nfilas = int(np.ceil(np.sqrt(len(trozos))))\ncolumnas = int(np.ceil(len(trozos) / filas))\n\npersonas_foto = 0\n\nplt.figure(figsize=(15,15))\nfor i, trozo in enumerate(trozos, 1):\n    ax = plt.subplot(3, 4, i)\n    ev_modelo = model(trozo)\n    imprimir_objetos_multiple(trozo, ev_modelo)\n    personas_foto = personas_foto + cantidad_personas(ev_modelo)\n\nprint(f'El total calculado de personas en la imagen es: {personas_foto}')\nprint(f'El total real de personas en la imagen es: {personas_real}')\n\n\n# Imagen que se desea dividir\nimagen_crowd = Image.fromarray(imagen_np.astype('uint8'))\nprint(imagen_crowd.size)\npersonas_real = number_list[7]\n\n# Tamaño de los trozos\ntamaño_trozos = (330, 126)\n\n# Obtiene los trozos de la imagen\ntrozos = dividir_imagen(imagen_crowd, tamaño_trozos)\n\n# Muestra cada trozo individualmente\nfor i, trozo in enumerate(trozos):\n    trozo.show()\n\n(990, 632)\n\n\n\nfilas = int(np.ceil(np.sqrt(len(trozos))))\ncolumnas = int(np.ceil(len(trozos) / filas))\n\npersonas_foto = 0\n\nplt.figure(figsize=(15,15))\nfor i, trozo in enumerate(trozos, 1):\n    ax = plt.subplot(5, 3, i)\n    ev_modelo = model(trozo)\n    imprimir_objetos_multiple(trozo, ev_modelo)\n    personas_foto = personas_foto + cantidad_personas(ev_modelo)\n\nprint(f'El total calculado de personas en la imagen es: {personas_foto}')\nprint(f'El total real de personas en la imagen es: {personas_real}')\n\nCon mejor resolución el resultado mejora considerablemente. Queda faltando entrenar el modulo con un dataset unicamente compuesto por personas, seguramente los resultados serian mucho mejores"
  },
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "Data analysis",
    "section": "",
    "text": "Why is Python so widely used in data analysis?\nThe rich ecosystem of libraries and tools specifically designed for data analysis is indeed the primary reason why Python is widely used in data analysis. Python’s popularity in this field is largely driven by the availability and quality of these libraries, which provide powerful and efficient solutions for various aspects of data manipulation, analysis, visualization, and machine learning.\nLibraries like NumPy, Pandas, Matplotlib, seaborn, scikit-learn, TensorFlow, and many others offer comprehensive functionalities that cover a wide range of data analysis tasks. These libraries are developed and maintained by dedicated teams of developers and have become essential tools for data analysts, scientists, and researchers worldwide."
  },
  {
    "objectID": "pandas.html",
    "href": "pandas.html",
    "title": "Pandas",
    "section": "",
    "text": "Pandas is a powerful and widely used open-source Python library for data manipulation and analysis. It provides easy-to-use data structures and functions to work with structured data, making it an essential tool for data scientists, analysts, and researchers.\nKey features of the Pandas library include:\n\nDataFrame: The primary data structure in Pandas is the DataFrame, which is a two-dimensional labeled data structure with columns of potentially different data types. It can be thought of as a spreadsheet or SQL table. DataFrames allow for easy manipulation and analysis of data, including filtering, sorting, grouping, and aggregation.\n\n\nimport pandas as pd\n\n# Creating a DataFrame from a dictionary\ndata = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n        'Age': [25, 30, 35, 40],\n        'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']}\ndf = pd.DataFrame(data)\nprint(df)\n\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\n\nReading Data from a File: Pandas can read data from various file formats such as CSV, Excel, JSON, SQL, and more.\n\n\n# Reading data from a CSV file\ndf = pd.read_csv('data.csv')\nprint(df.head())  # Print first few rows of the DataFrame\n\n\nBasic Data Manipulation: Pandas provides functions for data manipulation, including filtering, sorting, grouping, and aggregation.\n\n\n# Filtering rows based on a condition\nyoung_people = df[df['Age'] &lt; 30]\n\n# Sorting DataFrame by Age in descending order\ndf_sorted = df.sort_values(by='Age', ascending=False)\n\n# Grouping data by City and calculating average Age\navg_age_by_city = df.groupby('City')['Age'].mean()\n\n\nHandling Missing Values: Pandas provides functions to handle missing values, including checking for missing values, dropping rows with missing values, and filling missing values with a specified value.\n\n\n# Checking for missing values\nprint(df.isnull().sum())\n\n# Dropping rows with missing values\ndf_cleaned = df.dropna()\n\n# Filling missing values with a specified value\ndf_filled = df.fillna({'Age': df['Age'].mean()})\n\n\nExporting Data to a File: Pandas can export DataFrame to various file formats such as CSV, Excel, JSON, etc. python\n\n\n# Exporting DataFrame to a CSV file\ndf.to_csv('output.csv', index=False)\n\nOverall, Pandas is an essential tool for data analysis in Python, providing powerful data manipulation and analysis capabilities that streamline the process of working with structured data. Its intuitive syntax and extensive functionality make it a popular choice for data professionals."
  },
  {
    "objectID": "python/pandas.html",
    "href": "python/pandas.html",
    "title": "Pandas",
    "section": "",
    "text": "Pandas is a powerful and widely used open-source Python library for data manipulation and analysis. It provides easy-to-use data structures and functions to work with structured data, making it an essential tool for data scientists, analysts, and researchers.\nKey features of the Pandas library include:\n\nDataFrame: The primary data structure in Pandas is the DataFrame, which is a two-dimensional labeled data structure with columns of potentially different data types. It can be thought of as a spreadsheet or SQL table. DataFrames allow for easy manipulation and analysis of data, including filtering, sorting, grouping, and aggregation.\n\n\nimport pandas as pd\n\n# Creating a DataFrame from a dictionary\ndata = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n        'Age': [25, 30, 35, 40],\n        'City': ['New York', 'Los Angeles', 'Chicago', 'Houston']}\ndf = pd.DataFrame(data)\nprint(df)\n\n      Name  Age         City\n0    Alice   25     New York\n1      Bob   30  Los Angeles\n2  Charlie   35      Chicago\n3    David   40      Houston\n\n\n\nReading Data from a File: Pandas can read data from various file formats such as CSV, Excel, JSON, SQL, and more.\n\n\n# Reading data from a CSV file\ndf = pd.read_csv('data.csv')\nprint(df.head())  # Print first few rows of the DataFrame\n\n\nBasic Data Manipulation: Pandas provides functions for data manipulation, including filtering, sorting, grouping, and aggregation.\n\n\n# Filtering rows based on a condition\nyoung_people = df[df['Age'] &lt; 30]\n\n# Sorting DataFrame by Age in descending order\ndf_sorted = df.sort_values(by='Age', ascending=False)\n\n# Grouping data by City and calculating average Age\navg_age_by_city = df.groupby('City')['Age'].mean()\n\n\nHandling Missing Values: Pandas provides functions to handle missing values, including checking for missing values, dropping rows with missing values, and filling missing values with a specified value.\n\n\n# Checking for missing values\nprint(df.isnull().sum())\n\n# Dropping rows with missing values\ndf_cleaned = df.dropna()\n\n# Filling missing values with a specified value\ndf_filled = df.fillna({'Age': df['Age'].mean()})\n\n\nExporting Data to a File: Pandas can export DataFrame to various file formats such as CSV, Excel, JSON, etc. python\n\n\n# Exporting DataFrame to a CSV file\ndf.to_csv('output.csv', index=False)\n\nOverall, Pandas is an essential tool for data analysis in Python, providing powerful data manipulation and analysis capabilities that streamline the process of working with structured data. Its intuitive syntax and extensive functionality make it a popular choice for data professionals."
  },
  {
    "objectID": "python/python.html",
    "href": "python/python.html",
    "title": "Python",
    "section": "",
    "text": "Python is a popular and versatile programming language known for its simplicity and readability. Here’s a simple introduction highlighting the key elements to consider when using this programming language:\n\nReadability: Python focuses on code readability, meaning its syntax is clear and easy to understand. This makes it ideal for both beginners and experienced programmers.\nInterpreted: Python is an interpreted language, which means you don’t need to compile your code before running it. This makes writing and testing code easier, as you can see results immediately.\nDocumentation and Comments: Python supports inline comments and docstrings, allowing developers to document their code effectively. Writing clear and concise comments helps explain complex logic, assumptions, and intentions, enhancing readability and maintainability.\nDynamic typing: Python is dynamically typed, so you don’t need to explicitly declare a variable’s type. Python infers the data type during program execution.\n\n# Integer\nx = 10  \n# Output: &lt;class 'int'&gt;\nprint(type(x))  \n\n# String\nx = \"Hello\" \n# Output: &lt;class 'str'&gt; \nprint(type(x))  \n\n# List\nx = [1, 2, 3] \n# Output: &lt;class 'list'&gt; \nprint(type(x))                                           \n\nCross-platform: Python is compatible with various operating systems such as Windows, macOS, and Linux, making it highly portable.\nExtensive standard library: Python comes with an extensive standard library that provides modules and functions for a variety of tasks, from file manipulation to web development and data processing.\nObject-oriented: Python is an object-oriented programming language, meaning everything in Python is an object with properties and methods.\nIndentation: Indentation is the whitespace at the beginning of a line of code. Python uses indentation to delimit code blocks instead of braces or keywords like in other languages. This promotes writing clean and readable code.\n\nif x &gt; 5:\n    print(\"This is a Python code example\")\n    print(\"Great!\")\nelse:\n    print(\"Oh no!\")                                           \n\nActive community: Python has a large community of developers who contribute libraries, tutorials, and helpful resources. This means you can easily find help and access a wealth of educational resources."
  },
  {
    "objectID": "python/data_analysis.html",
    "href": "python/data_analysis.html",
    "title": "Data analysis",
    "section": "",
    "text": "Why is Python so widely used in data analysis?\nThe rich ecosystem of libraries and tools specifically designed for data analysis is indeed the primary reason why Python is widely used in data analysis. Python’s popularity in this field is largely driven by the availability and quality of these libraries, which provide powerful and efficient solutions for various aspects of data manipulation, analysis, visualization, and machine learning.\nLibraries like NumPy, Pandas, Matplotlib, seaborn, scikit-learn, TensorFlow, and many others offer comprehensive functionalities that cover a wide range of data analysis tasks. These libraries are developed and maintained by dedicated teams of developers and have become essential tools for data analysts, scientists, and researchers worldwide."
  },
  {
    "objectID": "python/scikit-learn.html",
    "href": "python/scikit-learn.html",
    "title": "Scikit-learn",
    "section": "",
    "text": "Scikit-learn is a popular machine learning library in Python that provides a simple and efficient tool for data mining and data analysis. It is built on top of other Python libraries like NumPy, SciPy, and Matplotlib, and it integrates well with other Python libraries for data manipulation and analysis.\nKey features of the scikit-learn library include:\n\nSupervised Learning: Scikit-learn provides algorithms for supervised learning tasks, including classification and regression. Here’s an example of using scikit-learn to train a classification model:\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Load the iris dataset\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2)\n\n# Initialize and train a K-Nearest Neighbors classifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\n# Evaluate the model\naccuracy = knn.score(X_test, y_test)\nprint(\"Accuracy:\", accuracy)\n\n\nUnsupervised Learning: Scikit-learn also provides algorithms for unsupervised learning tasks, such as clustering and dimensionality reduction. Here’s an example of using scikit-learn to perform clustering using K-Means:\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data\nX, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n\n# Apply K-Means clustering\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(X)\n\n# Visualize the clusters\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', color='red', s=100)\nplt.show()\n\n\nModel Evaluation and Validation: Scikit-learn provides tools for model evaluation and validation, such as cross-validation and performance metrics. Here’s an example of using scikit-learn to perform cross-validation on a classification model:\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n# Load the iris dataset\niris = load_iris()\n\n# Initialize a logistic regression classifier\nlog_reg = LogisticRegression()\n\n# Perform 5-fold cross-validation\nscores = cross_val_score(log_reg, iris.data, iris.target, cv=5)\nprint(\"Cross-Validation Scores:\", scores)\n\n\nHyperparameter Tuning: Scikit-learn provides tools for hyperparameter tuning to optimize model performance. Here’s an example of using scikit-learn’s GridSearchCV to perform hyperparameter tuning on a support vector machine (SVM) classifier:\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n# Define parameter grid\nparam_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['rbf', 'linear']}\n\n# Initialize SVM classifier\nsvm = SVC()\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(svm, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\n# Get the best parameters and score\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\nprint(\"Best Parameters:\", best_params)\nprint(\"Best Score:\", best_score)\n\nThese examples showcase some of the functionalities of scikit-learn for various machine learning tasks, including supervised and unsupervised learning, model evaluation and validation, and hyperparameter tuning. Scikit-learn’s simplicity and consistency make it a powerful tool for machine learning in Python."
  },
  {
    "objectID": "python/numpy.html",
    "href": "python/numpy.html",
    "title": "Numpy",
    "section": "",
    "text": "Numpy is a powerful Python library for numerical computing. It provides support for multidimensional arrays, mathematical functions, random number generation, linear algebra operations, and much more. NumPy is widely used in scientific computing, data analysis, and machine learning.\nKey features of the Numpy library include:\n\nCreating Numpy Arrays: Numpy’s primary data structure is the ndarray, a multidimensional array object. You can create Numpy arrays from Python lists or using Numpy’s array creation functions.\n\n\nimport numpy as np\n\n# Creating a 1D NumPy array from a Python list\narr1d = np.array([1, 2, 3, 4, 5])\n\n# Creating a 2D NumPy array from a list of lists\narr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\nprint(arr1d)\nprint(arr2d)\n\n[1 2 3 4 5]\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\n\n\n\nBasic Array Operations: Numpy provides functions and operators for basic array operations such as element-wise arithmetic operations, array indexing, slicing, and reshaping.\n\n\n# Element-wise arithmetic operations\narr1 = np.array([1, 2, 3])\narr2 = np.array([4, 5, 6])\n\nprint(arr1 + arr2)\nprint(arr1 * arr2)\n\n# Array indexing and slicing\nprint(arr2d[0, 1])  # Accessing element at row 0, column 1\nprint(arr2d[:, 1])  # Accessing all elements in column 1\n\n# Reshaping arrays\narr3d = np.arange(12).reshape((2, 2, 3))\nprint(arr3d)\n\n[5 7 9]\n[ 4 10 18]\n2\n[2 5 8]\n[[[ 0  1  2]\n  [ 3  4  5]]\n\n [[ 6  7  8]\n  [ 9 10 11]]]\n\n\n\nMathematical Functions: Numpy provides a wide range of mathematical functions that operate element-wise on arrays, such as sin, cos, exp, log, etc.\n\n\n# Computing trigonometric functions\narr = np.array([0, np.pi/2, np.pi])\nprint(np.sin(arr))\nprint(np.cos(arr))\n\n# Computing exponential and logarithmic functions\nprint(np.exp(arr))\nprint(np.log(arr))\n\n[0.0000000e+00 1.0000000e+00 1.2246468e-16]\n[ 1.000000e+00  6.123234e-17 -1.000000e+00]\n[ 1.          4.81047738 23.14069263]\n[      -inf 0.45158271 1.14472989]\n\n\n/tmp/ipykernel_37265/2324231433.py:8: RuntimeWarning: divide by zero encountered in log\n  print(np.log(arr))\n\n\n\nRandom Number Generation: Numpy provides functions for generating random numbers from various probability distributions.\n\n\n# Generating random numbers from a uniform distribution\nrand_arr = np.random.rand(3, 3)\nprint(rand_arr)\n\n# Generating random integers between a specified range\nrand_int = np.random.randint(1, 10, size=(3, 3))\nprint(rand_int)\n\n[[0.46235193 0.18655192 0.01109501]\n [0.94890837 0.58829727 0.61706833]\n [0.82129912 0.75495716 0.85936123]]\n[[1 2 9]\n [7 6 2]\n [3 5 4]]\n\n\n\nLinear Algebra Operations: Numpy provides functions for performing various linear algebra operations, such as matrix multiplication, matrix inversion, eigenvalue decomposition, etc.\n\n\n# Matrix multiplication\nA = np.array([[0, 2], [3, 4]])\nB = np.array([[4, 6], [7, 8]])\nprint(np.dot(A, B))\n\n# Matrix inversion\nA_inv = np.linalg.inv(A)\nprint(A_inv)\n\n[[14 16]\n [40 50]]\n[[-0.66666667  0.33333333]\n [ 0.5         0.        ]]\n\n\nNumpy’s extensive capabilities make it a fundamental library for numerical computing in Python, enabling efficient and scalable operations on arrays and matrices."
  },
  {
    "objectID": "python/tensorflow.html",
    "href": "python/tensorflow.html",
    "title": "TensorFlow",
    "section": "",
    "text": "TensorFlow is an open-source machine learning framework developed by Google. It allows users to build and deploy machine learning models, particularly deep learning models, efficiently. TensorFlow provides a flexible and scalable platform for training and deploying machine learning models across various domains, including computer vision, natural language processing, and reinforcement learning.\nKey features of the TensorFlow library include:\nimport tensorflow as tf\n\n# Define constants\na = tf.constant(2)\nb = tf.constant(3)\n\n# Define operations\nadd = tf.add(a, b)\nmul = tf.multiply(a, b)\n\n# Run the computational graph\nwith tf.Session() as sess:\n    result_add = sess.run(add)\n    result_mul = sess.run(mul)\n\nprint(\"Addition result:\", result_add)\nprint(\"Multiplication result:\", result_mul)\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense\n\n# Define a simple neural network\nmodel = Sequential([\n    Flatten(input_shape=(28, 28)),  # Flatten the input image\n    Dense(128, activation='relu'),  # Fully connected layer with ReLU activation\n    Dense(10, activation='softmax')  # Output layer with softmax activation\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_val, y_val))\n# Evaluate the model on test data\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(\"Test accuracy:\", test_acc)\n\n# Make predictions on new data\npredictions = model.predict(x_new_data)\nimport tensorflow_data_validation as tfdv\nimport tensorflow_transform as tft\nimport tensorflow_model_analysis as tfma\nimport tensorflow_model_server as tfms\n\n# Define and run a TFX pipeline\n# (Pipeline definition code goes here)\nThese examples showcase some of the functionalities of TensorFlow, including building computational graphs, training neural networks, deploying models, and using TensorFlow Extended for production ML pipelines. TensorFlow’s flexibility and scalability make it a powerful framework for machine learning and deep learning applications.\nIn some of the previus examples is also used Keras."
  },
  {
    "objectID": "python/tensorflow.html#keras",
    "href": "python/tensorflow.html#keras",
    "title": "TensorFlow",
    "section": "Keras",
    "text": "Keras\nKeras is an open-source deep learning library written in Python. It is designed to be user-friendly, modular, and easy to extend. Keras was developed with a focus on enabling fast experimentation and prototyping of deep neural networks.\nKey features of Keras include:\n\nSimple Interface: Keras provides a simple and consistent API that allows users to quickly build and train deep learning models without needing to write a lot of boilerplate code. It abstracts away many of the complexities of working with neural networks, making it accessible to both beginners and experienced practitioners.\n\nYou can see the difference to define and compile a model.\nUsing Keras:\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Define a sequential model\nmodel = Sequential()\n\n# Add layers to the model\nmodel.add(Dense(units=64, activation='relu', input_dim=100))\nmodel.add(Dense(units=10, activation='softmax'))\n\n# Compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nUsing TensorFlow:\n\nimport tensorflow as tf\n\n# Define placeholders for input and output\nx = tf.placeholder(tf.float32, shape=[None, 100])\ny = tf.placeholder(tf.float32, shape=[None, 10])\n\n# Define variables for weights and biases\nW = tf.Variable(tf.zeros([100, 64]))\nb = tf.Variable(tf.zeros([64]))\n\n# Define the forward pass\nlogits = tf.matmul(x, W) + b\noutput = tf.nn.softmax(logits)\n\n# Define loss function and optimizer\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\noptimizer = tf.train.AdamOptimizer().minimize(loss)\n\n# Initialize variables\ninit = tf.global_variables_initializer()\n\nKeras provides a higher-level abstraction for defining and training neural networks, with a more intuitive and concise interface. In the Keras example, we define a sequential model and add layers to it with a simple method call. TensorFlow, on the other hand, requires manual definition of placeholders, variables, and operations, which can be more verbose and less intuitive.\n\nModularity: Keras is built with a modular architecture, allowing users to easily create complex neural network architectures by combining simple building blocks called layers. These layers can be stacked together to create sequential models or combined in more complex ways using functional APIs.\n\nYou can see the difference to define a complex neural network.\nUsing Keras:\n\nfrom keras.layers import Input, Dense, concatenate\nfrom keras.models import Model\n\n# Define input layers\ninput1 = Input(shape=(10,))\ninput2 = Input(shape=(20,))\n\n# Define dense layers\ndense1 = Dense(64, activation='relu')(input1)\ndense2 = Dense(64, activation='relu')(input2)\n\n# Combine layers\ncombined = concatenate([dense1, dense2])\n\n# Define output layer\noutput = Dense(1, activation='sigmoid')(combined)\n\n# Define the model\nmodel = Model(inputs=[input1, input2], outputs=output)\n\nUsing TensorFlow:\n\nimport tensorflow as tf\n\n# Define input placeholders\ninput1 = tf.placeholder(tf.float32, shape=[None, 10])\ninput2 = tf.placeholder(tf.float32, shape=[None, 20])\n\n# Define variables for weights and biases\nW1 = tf.Variable(tf.random_normal([10, 64]))\nb1 = tf.Variable(tf.zeros([64]))\n\nW2 = tf.Variable(tf.random_normal([20, 64]))\nb2 = tf.Variable(tf.zeros([64]))\n\n# Define dense layers\ndense1 = tf.nn.relu(tf.matmul(input1, W1) + b1)\ndense2 = tf.nn.relu(tf.matmul(input2, W2) + b2)\n\n# Combine layers\ncombined = tf.concat([dense1, dense2], axis=1)\n\n# Define variables for output layer\nW_out = tf.Variable(tf.random_normal([128, 1]))\nb_out = tf.Variable(tf.zeros([1]))\n\n# Define output layer\noutput = tf.nn.sigmoid(tf.matmul(combined, W_out) + b_out)\n\nKeras provides a more intuitive and compact way to define complex neural network architectures by allowing the combination of layers using simple functions. TensorFlow requires manual definition of variables and operations for each layer and concatenation, which can be more cumbersome and error-prone.\n\nFlexibility: Keras provides support for both convolutional neural networks (CNNs) and recurrent neural networks (RNNs), as well as various other types of neural network architectures. It also supports multiple backends, including TensorFlow, Theano, and Microsoft Cognitive Toolkit (CNTK), giving users the flexibility to choose the backend that best suits their needs.\nIntegration with TensorFlow: In recent versions of TensorFlow, Keras has been integrated as the official high-level API for building and training deep learning models. This integration allows users to seamlessly switch between TensorFlow’s low-level operations and Keras’s high-level abstractions, leveraging the strengths of both frameworks.\n\nOverall, Keras is widely used in the deep learning community for its simplicity, flexibility, and ease of use. It has become a popular choice for building and training deep neural networks for a wide range of applications, including image recognition, natural language processing, and reinforcement learning.rks."
  },
  {
    "objectID": "python/matplotlib.html",
    "href": "python/matplotlib.html",
    "title": "Matplotlib",
    "section": "",
    "text": "Matplotlib is a comprehensive Python library for creating static, animated, and interactive visualizations. It provides a wide range of functionalities for producing plots and charts to visualize data in a clear and concise manner.\nKey features of the Matplotlib library include:\n\nLine Plot: A line plot is a basic type of plot that displays data points connected by straight line segments.\n\n\nimport matplotlib.pyplot as plt\n\n# Data\nx = [1, 2, 3, 4, 5]\ny = [2, 3, 5, 7, 11]\n\n# Create a line plot\nplt.plot(x, y)\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Line Plot')\nplt.show()\n\n\n\n\n\nScatter Plot: A scatter plot is used to visualize the relationship between two numerical variables by displaying individual data points.\n\n\n# Data\nx = [1, 2, 3, 4, 5]\ny = [2, 3, 5, 7, 11]\n\n# Create a scatter plot\nplt.scatter(x, y)\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.title('Scatter Plot')\nplt.show()\n\n\n\n\n\nHistogram: A histogram is used to represent the distribution of a continuous numerical variable by dividing the data into bins and displaying the frequency of observations in each bin.\n\n\n# Data\ndata = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5]\n\n# Create a histogram\nplt.hist(data, bins=5)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.title('Histogram')\nplt.show()\n\n\n\n\n\nBar Plot: A bar plot is used to compare categorical data by displaying the values of different categories as bars.\n\n\n# Data\ncategories = ['A', 'B', 'C', 'D']\nvalues = [10, 20, 15, 25]\n\n# Create a bar plot\nplt.bar(categories, values)\nplt.xlabel('Category')\nplt.ylabel('Value')\nplt.title('Bar Plot')\nplt.show()\n\n\n\n\n\nPie Chart: A pie chart is used to represent the proportions of different categories as slices of a circular pie.\n\n\n# Data\nsizes = [15, 30, 45, 10]\nlabels = ['A', 'B', 'C', 'D']\n\n# Create a pie chart\nplt.pie(sizes, labels=labels, autopct='%1.1f%%')\nplt.title('Pie Chart')\nplt.show()"
  },
  {
    "objectID": "python/seaborn.html",
    "href": "python/seaborn.html",
    "title": "Seaborn",
    "section": "",
    "text": "Seaborn is a Python visualization library based on Matplotlib that provides a high-level interface for creating attractive and informative statistical graphics. It is built on top of Matplotlib and integrates well with Pandas data structures, making it particularly useful for visualizing data stored in DataFrames.\nKey features of the Seaborn library include:\n\nScatter Plot with Regression Line: Seaborn makes it easy to create scatter plots with regression lines to visualize the relationship between two numerical variables.\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create scatter plot with regression line\nsns.regplot(x=\"total_bill\", y=\"tip\", data=tips)\nplt.xlabel('Total Bill')\nplt.ylabel('Tip')\nplt.title('Scatter Plot with Regression Line')\nplt.show()\n\n\nBox Plot: Seaborn provides a simple way to create box plots to visualize the distribution of numerical variables and detect outliers.\n\n\n# Create box plot\nsns.boxplot(x=\"day\", y=\"total_bill\", data=tips)\nplt.xlabel('Day')\nplt.ylabel('Total Bill')\nplt.title('Box Plot')\nplt.show()\n\n\nHistogram with Kernel Density Estimate (KDE): Seaborn can create histograms with kernel density estimates to visualize the distribution of numerical variables.\n\n\n# Create histogram with KDE\nsns.histplot(tips['total_bill'], kde=True)\nplt.xlabel('Total Bill')\nplt.ylabel('Frequency')\nplt.title('Histogram with KDE')\nplt.show()\n\n\nViolin Plot: Seaborn’s violin plots combine elements of box plots and kernel density estimates to visualize the distribution of numerical variables across different categories.\n\n\n# Create violin plot\nsns.violinplot(x=\"day\", y=\"total_bill\", data=tips)\nplt.xlabel('Day')\nplt.ylabel('Total Bill')\nplt.title('Violin Plot')\nplt.show()\n\n\nPair Plot: Seaborn’s pair plot creates a matrix of scatter plots for visualizing the relationships between multiple numerical variables in a DataFrame.\n\n\n# Create pair plot\nsns.pairplot(tips, hue=\"day\")\nplt.title('Pair Plot')\nplt.show()\n\nThese examples showcase some of the functionalities of Seaborn for creating attractive and informative statistical visualizations. Seaborn’s high-level interface and integration with Pandas make it a powerful tool for exploratory data analysis and presentation of statistical insights."
  },
  {
    "objectID": "machine_learning/machine_learning.html",
    "href": "machine_learning/machine_learning.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Machine learning is a subset of artificial intelligence (AI) that focuses on the development of algorithms and statistical models that enable computers to learn and improve their performance on a specific task without being explicitly programmed. In other words, it’s about creating algorithms that can learn patterns and relationships from data and make predictions or decisions based on that learning.\nThe core idea behind machine learning is to enable computers to learn from data and adapt their behavior accordingly. Instead of being explicitly programmed with rules or instructions to carry out a task, machine learning algorithms are trained on a dataset that contains examples of input-output pairs. By analyzing these examples, the algorithm learns patterns and relationships in the data, which it can then use to make predictions or decisions on new, unseen data.\nThere are several types of machine learning algorithms, including:\nSupervised learning: In supervised learning, the algorithm is trained on a labeled dataset, where each example is associated with a label or outcome variable. The goal is to learn a mapping from input variables to output variables, such as class labels or numerical values. Common supervised learning tasks include classification (predicting discrete class labels) and regression (predicting continuous values).\nUnsupervised learning: In unsupervised learning, the algorithm is trained on an unlabeled dataset, where the goal is to learn the underlying structure or patterns in the data. Unsupervised learning algorithms seek to identify clusters or groups of similar data points, discover hidden patterns, or reduce the dimensionality of the data.\nSemi-supervised learning: Semi-supervised learning combines elements of supervised and unsupervised learning. It leverages a small amount of labeled data along with a larger amount of unlabeled data to improve the performance of the learning algorithm.\nReinforcement learning: Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with an environment. The agent receives feedback or rewards based on its actions, and its goal is to learn a policy or strategy that maximizes the cumulative reward over time.\nMachine learning algorithms are applied to a wide range of real-world problems, including image and speech recognition, natural language processing, recommendation systems, autonomous vehicles, medical diagnosis, and financial forecasting, among others. As data availability and computing power continue to grow, machine learning is becoming increasingly important in solving complex problems and driving innovation across various industries.\nHere will be given a brief introduction to the most relevant aspects of supervised learning."
  },
  {
    "objectID": "machine_learning/supervised_intro.html",
    "href": "machine_learning/supervised_intro.html",
    "title": "Introduction to Supervised Learning",
    "section": "",
    "text": "To apply ML we need (clean and processed) data. To understand how it works, let’s check at an example of an imaginary store: we have data about temperature (ºC) and ice cream sales (€).\nThe data we use for creating (training) models is called training data.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame({\n    'temp (x)': [5, 8, 10, 12, 20, 23, 33, 35],\n    'sales (y)': [200, 235, 210, 240, 420, 435, 560, 620]})\ndf\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\n\n\n\n\n0\n5\n200\n\n\n1\n8\n235\n\n\n2\n10\n210\n\n\n3\n12\n240\n\n\n4\n20\n420\n\n\n5\n23\n435\n\n\n6\n33\n560\n\n\n7\n35\n620\n\n\n\n\n\n\n\nWe use the temperature for predicting sales.\nConventional names are:\n\nX for referring to the features/attributes that help in the prediction (temperature in our case).\ny for referring to the target attribute we want to predict (sales in our case).\n\n\n# temperature (ºC)\nX = df[['temp (x)']].values\n\n# sales (€)\ny = df['sales (y)'].values\n\n\ndef plot_data(X, y):\n    plt.scatter(X, y)\n    plt.xlabel('Temperature (ºC)')\n    plt.ylabel('Ice Cream Sales (€)')\n\nplot_data(X, y)\nplt.show()"
  },
  {
    "objectID": "machine_learning/supervised_intro.html#mae",
    "href": "machine_learning/supervised_intro.html#mae",
    "title": "Data",
    "section": "MAE",
    "text": "MAE\nMAE measures the average magnitude of the errors between predicted and observed (real) values.\n\\[\nMAE= \\frac{1}{N} \\sum|y_i-ŷ_i|\n\\]\nN is the number of observations.\n\n# Absolute error between each pair of observation-prediction\ndf['abs_error (|y - ŷ|)'] = abs(df['sales (y)'] - df['sales_pred (ŷ)'])\ndf\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\nsales_pred (ŷ)\nabs_error (|y - ŷ|)\n\n\n\n\n0\n5\n200\n170\n30\n\n\n1\n8\n235\n212\n23\n\n\n2\n10\n210\n240\n30\n\n\n3\n12\n240\n268\n28\n\n\n4\n20\n420\n380\n40\n\n\n5\n23\n435\n422\n13\n\n\n6\n33\n560\n562\n2\n\n\n7\n35\n620\n590\n30\n\n\n\n\n\n\n\n\n# MAE (sum of all elements divided by the length = mean)\nmae = df['abs_error (|y - ŷ|)'].mean()\nprint(f'MAE: {mae} €')\n\nMAE: 24.5 €\n\n\nFor simplicity, can be used a predefined function in sklearn for calculating MAE:\n\nfrom sklearn.metrics import mean_absolute_error\n\nmae = mean_absolute_error(df['sales (y)'], df['sales_pred (ŷ)'])\nprint(f'MAE: {mae} €')\n\nMAE: 24.5 €\n\n\n\n# Model 2\n\nw0_2 = 101\nw1_2 = 14.5\n\n# predicted y_2\ny_pred_model_2 = w0_2 + w1_2 * X\n\ndf['sales_pred (ŷ_2)'] = y_pred_model_2\ndf['abs_error (|y - ŷ_2|)'] = abs(df['sales (y)'] - df['sales_pred (ŷ_2)'])\ndf\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\nsales_pred (ŷ)\nabs_error (|y - ŷ|)\nsales_pred (ŷ_2)\nabs_error (|y - ŷ_2|)\n\n\n\n\n0\n5\n200\n170\n30\n173.5\n26.5\n\n\n1\n8\n235\n212\n23\n217.0\n18.0\n\n\n2\n10\n210\n240\n30\n246.0\n36.0\n\n\n3\n12\n240\n268\n28\n275.0\n35.0\n\n\n4\n20\n420\n380\n40\n391.0\n29.0\n\n\n5\n23\n435\n422\n13\n434.5\n0.5\n\n\n6\n33\n560\n562\n2\n579.5\n19.5\n\n\n7\n35\n620\n590\n30\n608.5\n11.5\n\n\n\n\n\n\n\n\nmae_2 = mean_absolute_error(df['sales (y)'], df['sales_pred (ŷ_2)'])\n\ndef plot_2_models_maes():\n    plt.scatter(X, y)\n    plt.plot(X, y_pred, marker='o', color='black', label=f\"$\\hat{{y}} = 100 + 14 * x$, MAE: {mae:.1f} €\")\n    plt.plot(X, y_pred_model_2, marker='o', color='orange', label=f\"$\\hat{{y}} = 101 + 14.5 * x$, MAE: {mae_2:.1f} €\")\n    plt.xlabel('Temperature (ºC)')\n    plt.ylabel('Ice Cream Sales (€)')\n    plt.legend(loc='upper left')\n\nplot_2_models_maes()\nplt.show()"
  },
  {
    "objectID": "machine_learning/supervised_intro.html#mse",
    "href": "machine_learning/supervised_intro.html#mse",
    "title": "Data",
    "section": "MSE",
    "text": "MSE\nMSE measures the average of the squares of the errors.\n\\[\nMSE= \\frac{1}{N} \\sum(y_i-ŷ_i)²\n\\]\nN is the number of observations.\nMSE is more sensitive to outliers than MAE.\nAn outlier is an observation that deviates significantly from other observations in a dataset. In other words, it’s a data point that lies outside the overall pattern of the data. Outliers can occur due to various reasons, such as measurement or recording errors, natural variation in the data, or they could represent genuinely different behavior in the data.\n\n# Absolute error between each pair of observation-prediction\ndf['abs_error_sq ((y - ŷ)²)'] = (df['sales (y)'] - df['sales_pred (ŷ)'])**2\ndf\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\nsales_pred (ŷ)\nabs_error (|y - ŷ|)\nabs_error_sq ((y - ŷ)²)\n\n\n\n\n0\n5\n200\n170\n30\n900\n\n\n1\n8\n235\n212\n23\n529\n\n\n2\n10\n210\n240\n30\n900\n\n\n3\n12\n240\n268\n28\n784\n\n\n4\n20\n420\n380\n40\n1600\n\n\n5\n23\n435\n422\n13\n169\n\n\n6\n33\n560\n562\n2\n4\n\n\n7\n35\n620\n590\n30\n900\n\n\n\n\n\n\n\n\n# MSE (sum of all elements divided by the length = mean)\nmse = df['abs_error_sq ((y - ŷ)²)'].mean()\nprint(f'MSE: {mse} €')\n\nMSE: 723.25 €\n\n\nFor simplicity, we could use a predefined function in sklearn for calculating MSE:\n\nfrom sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(df['sales (y)'], df['sales_pred (ŷ)'])\nprint(f'MSE: {mse} €²')\n\nMSE: 723.25 €²\n\n\n\n# Absolute error between each pair of observation-prediction_2\ndf['abs_error_sq ((y - ŷ_2)²)'] = (df['sales (y)'] - df['sales_pred (ŷ_2)'])**2\ndf\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\nsales_pred (ŷ)\nabs_error (|y - ŷ|)\nabs_error_sq ((y - ŷ)²)\nsales_pred (ŷ_2)\nabs_error (|y - ŷ_2|)\nabs_error_sq ((y - ŷ_2)²)\n\n\n\n\n0\n5\n200\n170\n30\n900\n173.5\n26.5\n702.25\n\n\n1\n8\n235\n212\n23\n529\n217.0\n18.0\n324.00\n\n\n2\n10\n210\n240\n30\n900\n246.0\n36.0\n1296.00\n\n\n3\n12\n240\n268\n28\n784\n275.0\n35.0\n1225.00\n\n\n4\n20\n420\n380\n40\n1600\n391.0\n29.0\n841.00\n\n\n5\n23\n435\n422\n13\n169\n434.5\n0.5\n0.25\n\n\n6\n33\n560\n562\n2\n4\n579.5\n19.5\n380.25\n\n\n7\n35\n620\n590\n30\n900\n608.5\n11.5\n132.25\n\n\n\n\n\n\n\n\n# MSE_2\nmse_2 = df['abs_error_sq ((y - ŷ_2)²)'].mean()\nprint(f'MSE: {mse_2} €²')\n\nMSE: 612.625 €²\n\n\n\ndef plot_2_models_mses():\n    plt.scatter(X, y)\n    plt.plot(X, y_pred, marker='o', color='black', label=f\"$\\hat{{y}} = 100 + 14 * x$, MSE: {mse:.1f} €²\")\n    plt.plot(X, y_pred_model_2, marker='o', color='orange', label=f\"$\\hat{{y}} = 101 + 14.5 * x$, MSE: {mse_2:.1f} €²\")\n    plt.xlabel('Temperature (ºC)')\n    plt.ylabel('Ice Cream Sales (€)')\n    plt.legend(loc='upper left')\n\nplot_2_models_mses()\nplt.show()"
  },
  {
    "objectID": "machine_learning/supervised_intro.html#mae-vs-mse",
    "href": "machine_learning/supervised_intro.html#mae-vs-mse",
    "title": "Data",
    "section": "MAE vs MSE",
    "text": "MAE vs MSE\nWhich is better?\nIt depends.\nProblem Sensitivity: * MAE if problem is not sensitive to large errors. * MSE if problem is sensitive to large errors.\nInterpretability: * MAE in the same units as target. * MSE not as interpretable (another option: RMSE).\nOptimization: * MSE for problems requiring gradient-based optimization."
  },
  {
    "objectID": "machine_learning/supervised_intro.html#data",
    "href": "machine_learning/supervised_intro.html#data",
    "title": "Introduction to Supervised Learning",
    "section": "",
    "text": "To apply ML we need (clean and processed) data. To understand how it works, let’s check at an example of an imaginary store: we have data about temperature (ºC) and ice cream sales (€).\nThe data we use for creating (training) models is called training data.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame({\n    'temp (x)': [5, 8, 10, 12, 20, 23, 33, 35],\n    'sales (y)': [200, 235, 210, 240, 420, 435, 560, 620]})\ndf\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\n\n\n\n\n0\n5\n200\n\n\n1\n8\n235\n\n\n2\n10\n210\n\n\n3\n12\n240\n\n\n4\n20\n420\n\n\n5\n23\n435\n\n\n6\n33\n560\n\n\n7\n35\n620\n\n\n\n\n\n\n\nWe use the temperature for predicting sales.\nConventional names are:\n\nX for referring to the features/attributes that help in the prediction (temperature in our case).\ny for referring to the target attribute we want to predict (sales in our case).\n\n\n# temperature (ºC)\nX = df[['temp (x)']].values\n\n# sales (€)\ny = df['sales (y)'].values\n\n\ndef plot_data(X, y):\n    plt.scatter(X, y)\n    plt.xlabel('Temperature (ºC)')\n    plt.ylabel('Ice Cream Sales (€)')\n\nplot_data(X, y)\nplt.show()"
  },
  {
    "objectID": "machine_learning/supervised_intro.html#linear-model",
    "href": "machine_learning/supervised_intro.html#linear-model",
    "title": "Introduction to Supervised Learning",
    "section": "Linear Model",
    "text": "Linear Model\nLine has the form:\n\\[\nf(x)=w_0 + w_1 · x\n\\]\nIf we asume:\n\\[\nw_0=100   \n\\] \\[    \nw_1=14\n\\]\nThe prediction of Ice cream sales is: \\[\nŷ=100+14x\n\\]\nwhere x is the temperature. We use ŷ for referring to the prediction given by our function approximation and y for referring to the real sales values in our data.\n\n# Model\n\nw0 = 100\nw1 = 14\n\n# predicted y\ny_pred = w0 + w1 * X\n\n\n# Model\n\nplot_data(X, y)\nplt.plot(X, y_pred, color='black', label=\"$\\hat{y} = 100 + 14 * x$\")\nplt.legend()\nplt.show()\n\n\n\n\n\n# Predictions\n\nplot_data(X, y)\nplt.plot(X, y_pred, marker='o', color='black', label=\"$\\hat{y} = 100 + 14 * x$\")\nplt.legend()\nplt.show()\n\n\n\n\n\ndf['sales_pred (ŷ)'] = y_pred\ndf\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\nsales_pred (ŷ)\n\n\n\n\n0\n5\n200\n170\n\n\n1\n8\n235\n212\n\n\n2\n10\n210\n240\n\n\n3\n12\n240\n268\n\n\n4\n20\n420\n380\n\n\n5\n23\n435\n422\n\n\n6\n33\n560\n562\n\n\n7\n35\n620\n590\n\n\n\n\n\n\n\nAre good the predictions good? It seems a reasonable model. But is this other model (orange) better or worse than the previous one (black)? Is needed a way to quantify how good the predictions are.\n\ndef plot_2_models(y1, y2):\n    plt.scatter(X, y)\n    plt.plot(X, y1, marker='o', color='black', label=\"$\\hat{y} = 100 + 14 * x$\")\n    plt.plot(X, y2, marker='o', color='orange', label=\"$\\hat{y} = 101 + 14.5 * x$\")\n    plt.xlabel('Temperature (ºC)')\n    plt.ylabel('Ice Cream Sales (€)')\n\n# Predictions 2\ny_pred_model_2 = 101 + 14.5 * X\nplot_2_models(y_pred, y_pred_model_2)"
  },
  {
    "objectID": "machine_learning/supervised_intro.html#metrics",
    "href": "machine_learning/supervised_intro.html#metrics",
    "title": "Introduction to Supervised Learning",
    "section": "Metrics",
    "text": "Metrics\nMetrics are used for estimating the error of our model. Two typical ML regression metrics (there are more):\n\nMean Absolute Error (MAE).\nMean Squared Error (MSE).\n\n\nMAE\nMAE measures the average magnitude of the errors between predicted and observed (real) values.\n\\[\nMAE= \\frac{1}{N} \\sum|y_i-ŷ_i|\n\\]\nN is the number of observations.\n\n# Absolute error between each pair of observation-prediction\ndf['abs_error (|y - ŷ|)'] = abs(df['sales (y)'] - df['sales_pred (ŷ)'])\ndf\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\nsales_pred (ŷ)\nabs_error (|y - ŷ|)\n\n\n\n\n0\n5\n200\n170\n30\n\n\n1\n8\n235\n212\n23\n\n\n2\n10\n210\n240\n30\n\n\n3\n12\n240\n268\n28\n\n\n4\n20\n420\n380\n40\n\n\n5\n23\n435\n422\n13\n\n\n6\n33\n560\n562\n2\n\n\n7\n35\n620\n590\n30\n\n\n\n\n\n\n\n\n# MAE (sum of all elements divided by the length = mean)\nmae = df['abs_error (|y - ŷ|)'].mean()\nprint(f'MAE: {mae} €')\n\nMAE: 24.5 €\n\n\nFor simplicity, can be used a predefined function in sklearn for calculating MAE:\n\nfrom sklearn.metrics import mean_absolute_error\n\nmae = mean_absolute_error(df['sales (y)'], df['sales_pred (ŷ)'])\nprint(f'MAE: {mae} €')\n\nMAE: 24.5 €\n\n\n\n# Model 2\n\nw0_2 = 101\nw1_2 = 14.5\n\n# predicted y_2\ny_pred_model_2 = w0_2 + w1_2 * X\n\ndf['sales_pred (ŷ_2)'] = y_pred_model_2\ndf['abs_error (|y - ŷ_2|)'] = abs(df['sales (y)'] - df['sales_pred (ŷ_2)'])\ndf\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\nsales_pred (ŷ)\nabs_error (|y - ŷ|)\nsales_pred (ŷ_2)\nabs_error (|y - ŷ_2|)\n\n\n\n\n0\n5\n200\n170\n30\n173.5\n26.5\n\n\n1\n8\n235\n212\n23\n217.0\n18.0\n\n\n2\n10\n210\n240\n30\n246.0\n36.0\n\n\n3\n12\n240\n268\n28\n275.0\n35.0\n\n\n4\n20\n420\n380\n40\n391.0\n29.0\n\n\n5\n23\n435\n422\n13\n434.5\n0.5\n\n\n6\n33\n560\n562\n2\n579.5\n19.5\n\n\n7\n35\n620\n590\n30\n608.5\n11.5\n\n\n\n\n\n\n\n\nmae_2 = mean_absolute_error(df['sales (y)'], df['sales_pred (ŷ_2)'])\n\ndef plot_2_models_maes():\n    plt.scatter(X, y)\n    plt.plot(X, y_pred, marker='o', color='black', label=f\"$\\hat{{y}} = 100 + 14 * x$, MAE: {mae:.1f} €\")\n    plt.plot(X, y_pred_model_2, marker='o', color='orange', label=f\"$\\hat{{y}} = 101 + 14.5 * x$, MAE: {mae_2:.1f} €\")\n    plt.xlabel('Temperature (ºC)')\n    plt.ylabel('Ice Cream Sales (€)')\n    plt.legend(loc='upper left')\n\nplot_2_models_maes()\nplt.show()\n\n\n\n\n\n\nMSE\nMSE measures the average of the squares of the errors.\n\\[\nMSE= \\frac{1}{N} \\sum(y_i-ŷ_i)²\n\\]\nN is the number of observations.\nMSE is more sensitive to outliers than MAE.\nAn outlier is an observation that deviates significantly from other observations in a dataset. In other words, it’s a data point that lies outside the overall pattern of the data. Outliers can occur due to various reasons, such as measurement or recording errors, natural variation in the data, or they could represent genuinely different behavior in the data.\n\n# Absolute error between each pair of observation-prediction\ndf['abs_error_sq ((y - ŷ)²)'] = (df['sales (y)'] - df['sales_pred (ŷ)'])**2\ndf\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\nsales_pred (ŷ)\nabs_error (|y - ŷ|)\nabs_error_sq ((y - ŷ)²)\n\n\n\n\n0\n5\n200\n170\n30\n900\n\n\n1\n8\n235\n212\n23\n529\n\n\n2\n10\n210\n240\n30\n900\n\n\n3\n12\n240\n268\n28\n784\n\n\n4\n20\n420\n380\n40\n1600\n\n\n5\n23\n435\n422\n13\n169\n\n\n6\n33\n560\n562\n2\n4\n\n\n7\n35\n620\n590\n30\n900\n\n\n\n\n\n\n\n\n# MSE (sum of all elements divided by the length = mean)\nmse = df['abs_error_sq ((y - ŷ)²)'].mean()\nprint(f'MSE: {mse} €')\n\nMSE: 723.25 €\n\n\nFor simplicity, we could use a predefined function in sklearn for calculating MSE:\n\nfrom sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(df['sales (y)'], df['sales_pred (ŷ)'])\nprint(f'MSE: {mse} €²')\n\nMSE: 723.25 €²\n\n\n\n# Absolute error between each pair of observation-prediction_2\ndf['abs_error_sq ((y - ŷ_2)²)'] = (df['sales (y)'] - df['sales_pred (ŷ_2)'])**2\ndf\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\nsales_pred (ŷ)\nabs_error (|y - ŷ|)\nabs_error_sq ((y - ŷ)²)\nsales_pred (ŷ_2)\nabs_error (|y - ŷ_2|)\nabs_error_sq ((y - ŷ_2)²)\n\n\n\n\n0\n5\n200\n170\n30\n900\n173.5\n26.5\n702.25\n\n\n1\n8\n235\n212\n23\n529\n217.0\n18.0\n324.00\n\n\n2\n10\n210\n240\n30\n900\n246.0\n36.0\n1296.00\n\n\n3\n12\n240\n268\n28\n784\n275.0\n35.0\n1225.00\n\n\n4\n20\n420\n380\n40\n1600\n391.0\n29.0\n841.00\n\n\n5\n23\n435\n422\n13\n169\n434.5\n0.5\n0.25\n\n\n6\n33\n560\n562\n2\n4\n579.5\n19.5\n380.25\n\n\n7\n35\n620\n590\n30\n900\n608.5\n11.5\n132.25\n\n\n\n\n\n\n\n\n# MSE_2\nmse_2 = df['abs_error_sq ((y - ŷ_2)²)'].mean()\nprint(f'MSE: {mse_2} €²')\n\nMSE: 612.625 €²\n\n\n\ndef plot_2_models_mses():\n    plt.scatter(X, y)\n    plt.plot(X, y_pred, marker='o', color='black', label=f\"$\\hat{{y}} = 100 + 14 * x$, MSE: {mse:.1f} €²\")\n    plt.plot(X, y_pred_model_2, marker='o', color='orange', label=f\"$\\hat{{y}} = 101 + 14.5 * x$, MSE: {mse_2:.1f} €²\")\n    plt.xlabel('Temperature (ºC)')\n    plt.ylabel('Ice Cream Sales (€)')\n    plt.legend(loc='upper left')\n\nplot_2_models_mses()\nplt.show()\n\n\n\n\n\n\nMAE vs MSE\nWhich is better?\nIt depends.\nProblem Sensitivity:\n\nMAE if problem is not sensitive to large errors.\nMSE if problem is sensitive to large errors.\n\nInterpretability:\n\nMAE in the same units as target.\nMSE not as interpretable (another option: RMSE).\n\nOptimization:\n\nMSE for problems requiring gradient-based optimization."
  },
  {
    "objectID": "machine_learning/Overfitting_underfitting.html",
    "href": "machine_learning/Overfitting_underfitting.html",
    "title": "Quality of Fit",
    "section": "",
    "text": "What happen if we found a funtion with MAE=0?\n\n\n\nImage"
  },
  {
    "objectID": "machine_learning/Overfitting_underfitting.html#overfitting",
    "href": "machine_learning/Overfitting_underfitting.html#overfitting",
    "title": "Quality of Fit",
    "section": "",
    "text": "What happen if we found a funtion with MAE=0?\n\n\n\nImage"
  },
  {
    "objectID": "machine_learning/linear_regression.html",
    "href": "machine_learning/linear_regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "The idea of the Linear Regression (LR) is fit a line to the data for making predictions:\n\\[\ny=w_0 + w_1·x\n\\]\nWe have to find the optimal values of the coefficients based on a metric. LR is optimazed on MSE by default in sklearn. We will consider the same example used for the intro.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame({\n    'temp (x)': [5, 8, 10, 12, 20, 23, 33, 35],\n    'sales (y)': [200, 235, 210, 240, 420, 435, 560, 620]})\ndf\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\n\n\n\n\n0\n5\n200\n\n\n1\n8\n235\n\n\n2\n10\n210\n\n\n3\n12\n240\n\n\n4\n20\n420\n\n\n5\n23\n435\n\n\n6\n33\n560\n\n\n7\n35\n620\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nX = df[['temp (x)']].values  # Feature\ny = df['sales (y)'].values    # Target\n\nlr = LinearRegression().fit(X, y)\nw0 = lr.intercept_\nw1 = lr.coef_[0]\nprint('w0:', round(w0, 2))\nprint('w1:', round(w1, 2))\n\ny_pred = w0 + w1 * X\n\nw0: 101.61\nw1: 14.43\n\n\n\n# Predictions\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_data(X, y):\n    plt.scatter(X, y)\n    plt.xlabel('Temperature (ºC)')\n    plt.ylabel('Ice Cream Sales (€)')\n\nplot_data(X, y)\nplt.plot(X, y_pred, marker='o', color='red', label=\"$\\hat{y} = 101.91 + 14.43 * x$\\nMAE = 22 €\\nMSE = 612 €²\")\nplt.legend()\nplt.show()\n\n\n\n\nsklearn models have a predict() function that automatically predicts values based on a trained model.\n\ny_pred = lr.predict(X)\n\n# Predictions\n\nplot_data(X, y)\nplt.plot(X, y_pred, marker='o', color='red', label=\"$\\hat{y} = 101.91 + 14.43 * x$\\nMAE = 22 €\\nMSE = 612 €²\")\nplt.legend()\nplt.show()\n\n\n\n\nThe results are the same."
  },
  {
    "objectID": "machine_learning/overfitting_underfitting.html",
    "href": "machine_learning/overfitting_underfitting.html",
    "title": "Quality of Fit",
    "section": "",
    "text": "What happen if we found a funtion with MAE=0?\n\nThe models are used for predicting future outcomes. When new data arrives, we know if our model is predicting it well or not. The new information can be used as test data, evaluating the model’s performance on data not seen during training.\nImagine the new data received is:\n\nimport pandas as pd\n\n\ndf_test = pd.DataFrame({\n    'temp (x)': [6, 15, 30],\n    'sales (y)': [205, 280, 520]\n})\ndf_test\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\n\n\n\n\n0\n6\n205\n\n\n1\n15\n280\n\n\n2\n30\n520\n\n\n\n\n\n\n\n\nBlack dots are the train data.\nRed dots are the test data.\nPolynomial model:\n\n\\[\n\\text{Train }MAE = 0€\n\\]\n\\[\n\\text{Test }MAE = 55€\n\\]\n\nPolynomial model: \\[\n\\text{Train }MAE = 0€\n\\]\n\\[\n\\text{Test }MAE = 55€\n\\]\nThe previus model: \\[\n\\text{Train }MAE = 22€\n\\]\n\\[\n\\text{Test }MAE = 23€\n\\]\n\nThis effect is called overfitting:\n\nOur polynomial model is too adapted to the training data.\nPerformance is much better on train data than on test data.\n\nThe linear model performs roughly the same on train and test.\n\nThe error is not very high.\nThis may be indicating that our model is giving good estimations."
  },
  {
    "objectID": "machine_learning/overfitting_underfitting.html#overfitting",
    "href": "machine_learning/overfitting_underfitting.html#overfitting",
    "title": "Quality of Fit",
    "section": "",
    "text": "What happen if we found a funtion with MAE=0?\n\nThe models are used for predicting future outcomes. When new data arrives, we know if our model is predicting it well or not. The new information can be used as test data, evaluating the model’s performance on data not seen during training.\nImagine the new data received is:\n\nimport pandas as pd\n\n\ndf_test = pd.DataFrame({\n    'temp (x)': [6, 15, 30],\n    'sales (y)': [205, 280, 520]\n})\ndf_test\n\n\n\n\n\n\n\n\ntemp (x)\nsales (y)\n\n\n\n\n0\n6\n205\n\n\n1\n15\n280\n\n\n2\n30\n520\n\n\n\n\n\n\n\n\nBlack dots are the train data.\nRed dots are the test data.\nPolynomial model:\n\n\\[\n\\text{Train }MAE = 0€\n\\]\n\\[\n\\text{Test }MAE = 55€\n\\]\n\nPolynomial model: \\[\n\\text{Train }MAE = 0€\n\\]\n\\[\n\\text{Test }MAE = 55€\n\\]\nThe previus model: \\[\n\\text{Train }MAE = 22€\n\\]\n\\[\n\\text{Test }MAE = 23€\n\\]\n\nThis effect is called overfitting:\n\nOur polynomial model is too adapted to the training data.\nPerformance is much better on train data than on test data.\n\nThe linear model performs roughly the same on train and test.\n\nThe error is not very high.\nThis may be indicating that our model is giving good estimations."
  },
  {
    "objectID": "machine_learning/overfitting_underfitting.html#underfitting",
    "href": "machine_learning/overfitting_underfitting.html#underfitting",
    "title": "Quality of Fit",
    "section": "Underfitting",
    "text": "Underfitting\nThe underfitting happens when the model performs poorly on both train and test data. We have underfitting when the model is not capturing underlying patterns of the data."
  },
  {
    "objectID": "machine_learning/train_test.html",
    "href": "machine_learning/train_test.html",
    "title": "Train/Test",
    "section": "",
    "text": "Waiting for new data before evaluating our model would be a waste of time. We can emulate unseen data by splitting our data into train and test sets:\n\nUse the train set for fitting our model.\nUse the test set for evaluating our model’s performance\n\nUsually test sizes are 10-30% of the initial data, but it depends from the size of the dataset:\n\nLarge datasets may require a smaller percentage (e.g., 10-20%).\nSmaller datasets may need a larger test set to ensure that it is representative (e.g., 30-40%).\n\nSimple ML:\n\nUse train_test_split from sklearn for splitting the data:\n\nX_train: features used for training the model.\nX_test: features used for testing the model’s performance.\ny_train: target for the train.\ny_test: target for the test.\n\nFit a model to the train set.\nCheck model’s performance on train and test sets.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame({\n    'temp (x)': [5, 8, 10, 12, 20, 23, 33, 35],\n    'sales (y)': [200, 235, 210, 240, 420, 435, 560, 620]})\n\nX = df[['temp (x)']].values  # Feature\ny = df['sales (y)'].values    # Target\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\n# Using 20% of the data for testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n\n# Fit LR to train data (search for the optimal line based on MSE)\nlr = LinearRegression().fit(X_train, y_train)\n\n# Predicted values on the test set\ny_pred_train = lr.predict(X_train)\ny_pred_test = lr.predict(X_test)\n\n# Calculate train and test MAE\nmae_train = mean_absolute_error(y_train, y_pred_train).round(0).astype(int)\nmae_test = mean_absolute_error(y_test, y_pred_test).round(0).astype(int)\n\n# Predictions\nplt.scatter(X_train, y_train, color='black')\nplt.scatter(X_test, y_test, color='r')\nplt.plot(X_train, y_pred_train, marker='o', label=f\"MAE train = {mae_train} €\\nMAE test = {mae_test} €\")\nplt.xlabel('Temperature (ºC)')\nplt.ylabel('Ice Cream Sales (€)')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "machine_learning/tree_methods.html#random-forest",
    "href": "machine_learning/tree_methods.html#random-forest",
    "title": "Tree-Based Methods",
    "section": "Random Forest",
    "text": "Random Forest"
  },
  {
    "objectID": "machine_learning/tree_methods.html#gradient-boosted-decision-trees-gbdt",
    "href": "machine_learning/tree_methods.html#gradient-boosted-decision-trees-gbdt",
    "title": "Tree-Based Methods",
    "section": "Gradient Boosted Decision Trees (GBDT)",
    "text": "Gradient Boosted Decision Trees (GBDT)"
  },
  {
    "objectID": "colab/colab_intro.html",
    "href": "colab/colab_intro.html",
    "title": "Colab",
    "section": "",
    "text": "What is Google Colab?\nGoogle Colab, short for Google Colaboratory, is a cloud-based platform provided by Google that offers free Jupyter notebooks with GPU and TPU (Tensor Processing Unit) support. It allows users to write and execute Python code collaboratively, directly in the browser, without requiring any setup or installation.\nSome key features of Google Colab include:\n\nFree access to GPUs and TPUs: Google Colab provides free access to Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs), which are hardware accelerators commonly used for training machine learning models and processing large datasets.\nJupyter Notebooks integration: Colab supports Jupyter notebooks, which allow users to write and execute code in a cell-based format, making it easy to write, run, and visualize code and results.\nCollaboration and sharing: Colab allows multiple users to collaborate on the same notebook simultaneously, similar to Google Docs. Users can share their notebooks with others, making it easy to collaborate on projects or share code and analyses.\nIntegration with Google Drive: Colab notebooks are stored in Google Drive, allowing users to access and manage their notebooks directly from their Google Drive account.\nSupport for popular libraries and frameworks: Colab comes pre-installed with many popular Python libraries and frameworks, including TensorFlow, Keras, Pandas, Numpy and scikit-learn, making it suitable for a wide range of machine learning and data analysis tasks.\n\nOverall, Google Colab provides a convenient and powerful environment for writing, running, and sharing code, particularly for tasks involving machine learning and data analysis, thanks to its access to GPUs and TPUs.\nLimitations and restrictions\nGoogle Colab provides free access to computational resources, including CPU, GPU, and TPU, but there are limitations and restrictions on the usage of these resources:\n\nSession Timeout: Colab sessions have a maximum duration of 12 hours. After this period, the session will automatically disconnect, and any unsaved work will be lost. Users can reconnect to a new session, but they will lose access to any temporary files or variables stored in the previous session.\nIdle Timeout: If a Colab notebook remains idle for a certain period, typically around 30 minutes, the session may disconnect automatically. This idle timeout is in place to conserve resources and ensure fair usage of the platform.\nResource Limits: While Colab offers free access to GPU and TPU resources, there are usage limits and restrictions on the availability of these resources. For example, the free version of Colab may limit the maximum amount of GPU memory available, restrict access to certain GPU models, and impose quotas on the total usage of GPU and TPU resources.\nMemory Limits: Colab imposes memory limits on the amount of RAM available to a notebook. The exact limit may vary depending on the type of virtual machine allocated to the session but typically ranges from 12 GB to 25 GB.\nStorage Limits: Colab notebooks are stored in Google Drive, which provides limited storage space for free Google accounts. Users may encounter storage limits if they exceed the allocated quota for Google Drive storage.\nConcurrency Limits: Colab may limit the number of concurrent sessions or active sessions per user. Users may experience delays or restrictions in accessing resources during periods of high demand.\nNetwork Access: Colab restricts outbound network access for security reasons. Users may encounter limitations when trying to access external resources or services, such as databases, APIs, or websites.\n\nIt’s essential to be aware of these limitations when using Google Colab, especially for tasks that require extended computation or large amounts of data. While Colab provides valuable resources for experimentation, prototyping, and learning, users may need to consider alternative options or paid cloud services for more intensive or long-running workloads.\nColab Pro\nGoogle Colab offers a paid version called “Colab Pro” that provides additional features and benefits compared to the free version. Here are some key aspects of Colab Pro:\n\nIncreased Resource Limits: Colab Pro users have access to higher resource limits compared to the free version. This includes higher GPU memory limits, faster GPU models, and potentially longer session durations.\nPriority Access: Colab Pro users may receive priority access to resources, which can reduce wait times and ensure faster execution of tasks, especially during periods of high demand.\nLonger Session Durations: Colab Pro users may have access to longer session durations compared to the free version, allowing them to run computations and experiments for extended periods without interruption.\nAdditional Storage: Colab Pro users may receive additional storage space for Google Drive, which can be useful for storing larger datasets, files, or notebooks.\nNo Idle Timeout: Colab Pro sessions may not be subject to the idle timeout restrictions imposed on the free version, allowing users to maintain active sessions for longer periods without disconnection.\nTechnical Support: Colab Pro users may receive access to priority technical support from Google, which can be beneficial for resolving issues or troubleshooting problems quickly.\nCustomization and Control: Colab Pro users may have access to additional customization options and control over their computing environment, allowing them to tailor the environment to their specific needs and preferences.\n\nColab Pro is a subscription-based service, and users are billed on a monthly basis. The pricing and availability of Colab Pro may vary depending on factors such as region and currency. Users can sign up for Colab Pro directly through the Google Colab website or the Google Cloud Platform console.\nOverall, Colab Pro offers a range of benefits and features that can enhance the user experience and productivity, especially for users with demanding computational requirements or those who require additional resources and support."
  },
  {
    "objectID": "neural_networks/neuralNetworks.html",
    "href": "neural_networks/neuralNetworks.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Neural Networks\nNeural networks, often referred to as artificial neural networks (ANNs) or simply “neural nets,” are computational models inspired by the structure and function of biological neural networks in the human brain. They are a subset of machine learning algorithms designed to recognize patterns and learn from data to perform tasks such as classification, regression, clustering, and more.\nThe neural networks are not easily interpretable, but have a very good performance on image and speech recognition, natural language processing, computer vision, and reinforcement learning.\nNeurons: Neurons are the basic building blocks of neural networks. Each neuron receives one or more input signals, applies a transformation to those inputs, and produces an output signal. In artificial neural networks, neurons are typically represented as mathematical functions that compute a weighted sum of inputs and apply an activation function to produce the output.\n\nThe inputs are the values received from the input data (or from the ouput of neurons in previous layer). Each input is associated with a weight, wich represents the strength of the connection between the input and the neuron.\nAfter calculating the weighted sum of inputs the neuron applies an activation function f to produce the output, which is nothing more than the prediction of the neuron.\nNeural networks have forward and backward propagation of the information to calculate the values of the weights asociated to every input data:\n\nForward propagation: for each input row the features (inputs) are multiplied with their corresponding weights and added. An activation function is applied to this sum. At the end is calculated the loss function (prediction error).\nBackward propagation: Each weight is updated to optimize the loss function.\nContinue applying forward and backward propagation until convergense of the weights of until the maximum number of iteration (decided by user).\n\nActivation Functions: Activation functions introduce non-linearity into the neural network, allowing it to learn complex relationships and patterns in the data. Common activation functions include: - linear - sigmoid - tanh - ReLU (Rectified Linear Unit) - softmax\nThe choice of activation function determines the characteristics of the neuron’s output. Different activation functions introduce different properties such as non-linearity, sparsity, or output range.\nThe choice of activation function depends on the specific task, network architecture, and desired properties of the network output. Each activation function has its advantages and disadvantages, and the selection is often based on empirical evaluation and experimentation.\nLayers: Neurons are organized into layers within a neural network. The three main types of layers are:\n\nInput Layer: The input layer receives input data and passes it to the subsequent layers. It has the same length as number of features.\nHidden Layers: Hidden layers are intermediate layers between the input and output layers. They perform transformations on the input data to learn patterns and features. The hidden layers can have any lenght.\nOutput Layer: The output layer produces the final output of the neural network, such as class probabilities in classification tasks or numerical values in regression tasks. The lenght of this layer depends on the type of problem being studied. For example one neuron is needed for regression problems, or N neurons are needed for classifications problems with N classes.\n\n\nDeep NN (DNN): A deep neural network (DNN) is a type of artificial neural network with multiple hidden layers between the input and output layers. It is characterized by its depth, referring to the number of hidden layers in the network. Deep neural networks are capable of learning complex patterns and representations from data, making them suitable for a wide range of tasks, including image recognition, natural language processing or speech recognition.\n\nLoss function: The loss function measures the discrepancy between the predicted output of a neural network and the actual target values in the training data. It quantifies how well the neural network is performing on a specific task and provides a measure of the error or loss incurred by the model’s predictions.\nThe choice of loss function depends on the type of task the neural network is performing.\nFor Regression problems:\n\nMean Squared Error (MSE): \\[\nMSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\nMean Absolute Error (MAE): \\[\nMAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n\\]\nHuber loss (robust to outliers): \\[\nHuber Loss = \\frac{1}{n} \\sum_{i=1}^{n} \\left\\{ \\begin{array}{ll}\n\\frac{1}{2} (y_i - \\hat{y}_i)^2 & \\text{for } |y_i - \\hat{y}_i| \\leq \\delta \\\\\n\\delta |y_i - \\hat{y}_i| - \\frac{1}{2} \\delta^2 & \\text{for } |y_i - \\hat{y}_i| &gt; \\delta\n\\end{array} \\right.\n\\]\n\nFor Classification problems:\n\nBinary Cross-Entropy: \\[\nBinary Cross-Entropy = -\\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right)\n\\]\nSparse Categorical Cross-Entropy: \\[\nSparse Categorical Cross-Entropy = -\\frac{1}{N} \\sum_{i=1}^{N} \\log(p_{i, y_i})\n\\]\nCategorical Cross-Entropy: \\[\nCategorical Cross-Entropy = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{m} y_{ij} \\log(\\hat{y}_{ij})\n\\]\n\nThe idea is to optimize the loss function, but them do not always converge. There is a considerable risk to get stuck in a local minima and never get the global minima.\n\nTo avoid this we use Optimizers: - Adam - RMSprop - AdaGrad\n(Adam is an effective optimizer in the 95% of cases)\nEpochs: an epoch refers to one complete pass through the entire training dataset during the training process. During each epoch, the model sees the entire dataset, computes the loss, and updates the model parameters (weights and biases) based on the gradients of the loss function.\nNeural Networks require multiple epochs for a good optimization of the weights, but this number of epochs has to be decided carefully: - Too many epochs can lead to overfitting. - Too few epochs may result in underfitting.\nRecomendations\nHere are some recomendations for the definition and training of the model.\n\nNumber of layers:\n\nOne or two hidden for starting\nMaybe deeper for more complex data os task\n\nNeurons per layer:\n\nIt is common to start with a lenght between the input and the ouput\nOr even with more neurons than the input on the first layer\nReducing the hidden sizes as the network goes deeper\n\nAdam is normally a good start"
  }
]